{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec526712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "import polars as pl\n",
    "\n",
    "conn=pyodbc.connect(\n",
    "    Trusted_Connection=r'Yes',\n",
    "    Driver=r'{ODBC Driver 17 for SQL Server}',\n",
    "    Server=r'DESKTOP-QQPNCCN\\SQLEXPRESS',\n",
    "    Database=r'SQL Tutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2860f4-1ec6-454a-a985-dd980a361b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31761f38-16fd-4511-b7cf-6987df7db076",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5ac0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 chunk [00:13,  1.32s/ chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 chunk [00:26,  1.33s/ chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 chunk [00:39,  1.33s/ chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 3000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33 chunk [00:43,  1.30s/ chunk]\n"
     ]
    }
   ],
   "source": [
    "def read_sql_query_in_chunks(query, conn, chunksize=100000):\n",
    "    total_rows = 0\n",
    "    for chunk in tqdm(pd.read_sql_query(query, conn, chunksize=chunksize), unit=\" chunk\"):\n",
    "        total_rows += len(chunk)\n",
    "        if total_rows % 1000000 == 0:\n",
    "            print(f\"Read {total_rows} rows\")\n",
    "        yield chunk\n",
    "\n",
    "query=\"Select * From [new].dbo.lung_data\" \n",
    "\n",
    "df=pd.concat(read_sql_query_in_chunks(query, conn, chunksize=100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f614bdc-5f6a-483a-a2f3-7fdc75cd569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3250000 entries, 0 to 49999\n",
      "Columns: 18 entries, id to survived\n",
      "dtypes: bool(6), float64(2), int64(2), object(8)\n",
      "memory usage: 1.4 GB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose = False, memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac8e03-407a-4709-a1a7-2d935fd6fd47",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794bb3cd-89a1-4b64-8dcb-41cc740a95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = (r'mssql+pyodbc:///?odbc_connect=DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "       r'SERVER=DESKTOP-QQPNCCN\\SQLEXPRESS;DATABASE=new;Trusted_Connection=yes;')\n",
    "\n",
    "# Specify the table name and index column\n",
    "table_name = \"lung_data\"\n",
    "index_col = \"id\" #Column which becomes the index, and defines the partitioning\n",
    "\n",
    "# Create a Dask dataframe by reading from SQL table\n",
    "ddf = dd.read_sql_table(table_name, uri, index_col=index_col,npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1816e187-40fd-4a8f-9aa8-823b87fe1e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 540.99 MB\n"
     ]
    }
   ],
   "source": [
    "memory_usage = ddf.memory_usage(deep=True).compute().sum()\n",
    "print(f\"Memory usage: {round(memory_usage / (1024 ** 2),2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa71251-b4d4-4a7e-8521-683141bb3fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 565.79 MB\n"
     ]
    }
   ],
   "source": [
    "dask_df = dd.from_pandas(df, npartitions=10)\n",
    "memory_usage = dask_df.memory_usage(deep=True).compute().sum()\n",
    "print(f\"Memory usage: {round(memory_usage / (1024 ** 2),2)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edac83b-10a2-4af9-9396-e4104557a2da",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2106c-118e-4c18-8b67-2715c00440cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 batch [00:14,  1.45s/ batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 batch [00:16,  1.45s/ batch]"
     ]
    }
   ],
   "source": [
    "def read_sql_query_in_batches(query, conn, batch_size=100000):\n",
    "    total_rows = 0\n",
    "    for batch in tqdm(pl.read_database(query,conn,batch_size=100000,iter_batches=True), unit=\" batch\"):\n",
    "        total_rows += len(batch)\n",
    "        if total_rows % 1000000 == 0:\n",
    "            print(f\"Read {total_rows} rows\")\n",
    "        yield batch\n",
    "query=\"Select * From [new].dbo.lung_data\" \n",
    "\n",
    "\n",
    "pdf = pl.concat(read_sql_query_in_batches(query, conn, batch_size=100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5f4db-95b6-48b5-a296-b81f3b34b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory usage: {round(pdf.estimated_size('mb'),2)} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
